{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# console : mlflow ui\n",
    "# AccÃ¨s console MLFlow : http://127.0.0.1:5000\n",
    "# 1 - Chargement & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\env_p7_MLOps\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\motar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\env_p7_MLOps\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "âœ… Toutes les librairies sont prÃ©sentes et prÃªtes Ã  Ãªtre utilisÃ©es !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\motar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\motar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\motar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/motar/Desktop/1-openclassrooms/AI_Engineer/1-projets/P07/P7_sentiment_MLOps/notebooks/mlruns/906586012259731436', creation_time=1742576058928, experiment_id='906586012259731436', last_update_time=1742576058928, lifecycle_stage='active', name='Sentiment Analysis Project', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "from requirements import *\n",
    "from src import data_preprocessing as dp\n",
    "from src import model_training as mt\n",
    "from src import evaluate as ev\n",
    "from src import utils\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_experiment(\"Sentiment Analysis Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Affichage de la structure dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”œâ”€â”€ .git\n",
      "â”œâ”€â”€ â”œâ”€â”€ COMMIT_EDITMSG\n",
      "â”œâ”€â”€ â”œâ”€â”€ FETCH_HEAD\n",
      "â”œâ”€â”€ â”œâ”€â”€ HEAD\n",
      "â”œâ”€â”€ â”œâ”€â”€ config\n",
      "â”œâ”€â”€ â”œâ”€â”€ description\n",
      "â”œâ”€â”€ â”œâ”€â”€ hooks\n",
      "â”œâ”€â”€ â”œâ”€â”€ index\n",
      "â”œâ”€â”€ â”œâ”€â”€ info\n",
      "â”œâ”€â”€ â”œâ”€â”€ logs\n",
      "â”œâ”€â”€ â”œâ”€â”€ objects\n",
      "â”œâ”€â”€ â””â”€â”€ refs\n",
      "â”œâ”€â”€ .gitattributes\n",
      "â”œâ”€â”€ .gitignore\n",
      "â”œâ”€â”€ README.md\n",
      "â”œâ”€â”€ data\n",
      "â”œâ”€â”€ â””â”€â”€ tweets.csv\n",
      "â”œâ”€â”€ env_p7_MLOps\n",
      "â”œâ”€â”€ â”œâ”€â”€ Include\n",
      "â”œâ”€â”€ â”œâ”€â”€ Lib\n",
      "â”œâ”€â”€ â”œâ”€â”€ Scripts\n",
      "â”œâ”€â”€ â”œâ”€â”€ etc\n",
      "â”œâ”€â”€ â”œâ”€â”€ pyvenv.cfg\n",
      "â”œâ”€â”€ â””â”€â”€ share\n",
      "â”œâ”€â”€ logs\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741361244.PC-ARNAUD.37024.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741362034.PC-ARNAUD.9628.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741364804.PC-ARNAUD.38328.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741507476.PC-ARNAUD.3024.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741507646.PC-ARNAUD.3024.1\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741513667.PC-ARNAUD.13340.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741599815.PC-ARNAUD.27736.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741603247.PC-ARNAUD.3852.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741603852.PC-ARNAUD.24400.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741604981.PC-ARNAUD.29024.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741609885.PC-ARNAUD.30608.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741633603.PC-ARNAUD.40828.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741878760.PC-ARNAUD.26112.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741896036.PC-ARNAUD.26288.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741935846.PC-ARNAUD.7788.0\n",
      "â”œâ”€â”€ â”œâ”€â”€ events.out.tfevents.1741941545.PC-ARNAUD.7788.2\n",
      "â”œâ”€â”€ â””â”€â”€ events.out.tfevents.1742199199.PC-ARNAUD.25320.0\n",
      "â”œâ”€â”€ mlflow\n",
      "â”œâ”€â”€ mlruns\n",
      "â”œâ”€â”€ â”œâ”€â”€ .trash\n",
      "â”œâ”€â”€ â”œâ”€â”€ 0\n",
      "â”œâ”€â”€ â”œâ”€â”€ 346150508169324204\n",
      "â”œâ”€â”€ â”œâ”€â”€ 968426534980835769\n",
      "â”œâ”€â”€ â””â”€â”€ models\n",
      "â”œâ”€â”€ models\n",
      "â”œâ”€â”€ models_saved\n",
      "â”œâ”€â”€ â”œâ”€â”€ X_bow.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ X_fasttext.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ X_tfidf.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ X_use.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ cleaned_tweets.csv\n",
      "â”œâ”€â”€ â”œâ”€â”€ cleaned_tweets.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ cosine_similarity.npy\n",
      "â”œâ”€â”€ â”œâ”€â”€ distilbert_dataset.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ distilbert_eval_results.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ distilbert_model\n",
      "â”œâ”€â”€ â”œâ”€â”€ distilbert_output\n",
      "â”œâ”€â”€ â”œâ”€â”€ distilbert_results\n",
      "â”œâ”€â”€ â”œâ”€â”€ lgbm_model.txt\n",
      "â”œâ”€â”€ â”œâ”€â”€ log_reg_model.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ logs\n",
      "â”œâ”€â”€ â”œâ”€â”€ lstm_eval_simulation.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ lstm_model.h5\n",
      "â”œâ”€â”€ â”œâ”€â”€ rf_model.pkl\n",
      "â”œâ”€â”€ â”œâ”€â”€ tfidf_split_data.joblib\n",
      "â”œâ”€â”€ â”œâ”€â”€ tokenized_distilbert_dataset\n",
      "â”œâ”€â”€ â”œâ”€â”€ tweets_fasttext.txt\n",
      "â”œâ”€â”€ â”œâ”€â”€ vader_scores.pkl\n",
      "â”œâ”€â”€ â””â”€â”€ y_use.pkl\n",
      "â”œâ”€â”€ notebooks\n",
      "â”œâ”€â”€ â”œâ”€â”€ 01_full_pipeline_modeling.ipynb\n",
      "â”œâ”€â”€ â”œâ”€â”€ P7_main_notebook.ipynb\n",
      "â”œâ”€â”€ â””â”€â”€ mlruns\n",
      "â”œâ”€â”€ src\n",
      "â”œâ”€â”€ â”œâ”€â”€ __init__.py\n",
      "â”œâ”€â”€ â”œâ”€â”€ __pycache__\n",
      "â”œâ”€â”€ â”œâ”€â”€ api\n",
      "â”œâ”€â”€ â”œâ”€â”€ data_preprocessing.py\n",
      "â”œâ”€â”€ â”œâ”€â”€ evaluate.py\n",
      "â”œâ”€â”€ â”œâ”€â”€ functions.py\n",
      "â”œâ”€â”€ â”œâ”€â”€ model_training.py\n",
      "â”œâ”€â”€ â”œâ”€â”€ requirements.py\n",
      "â”œâ”€â”€ â”œâ”€â”€ requirements.txt\n",
      "â”œâ”€â”€ â””â”€â”€ utils.py\n",
      "â”œâ”€â”€ tests\n",
      "â”œâ”€â”€ â””â”€â”€ test_api.py\n",
      "â”œâ”€â”€ tmp_trainer\n",
      "â”œâ”€â”€ â””â”€â”€ runs\n",
      "â””â”€â”€ tokenized_distilbert_dataset\n",
      "â”œâ”€â”€ â”œâ”€â”€ cache-4fd5ed254c63e60d.arrow\n",
      "â”œâ”€â”€ â”œâ”€â”€ cache-59508564038c63f4.arrow\n",
      "â”œâ”€â”€ â”œâ”€â”€ cache-93488f2cd6523051.arrow\n",
      "â”œâ”€â”€ â”œâ”€â”€ cache-b4c7a2967748401c.arrow\n",
      "â”œâ”€â”€ â”œâ”€â”€ cache-bb6997e2954fe2b5.arrow\n",
      "â”œâ”€â”€ â”œâ”€â”€ data-00000-of-00001.arrow\n",
      "â”œâ”€â”€ â”œâ”€â”€ dataset_info.json\n",
      "â”œâ”€â”€ â””â”€â”€ state.json\n"
     ]
    }
   ],
   "source": [
    "utils.afficher_structure_dossier(\"..\", max_niveaux = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Chargement des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ RÃ©pertoire courant : c:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(f\"ğŸ“‚ RÃ©pertoire courant : {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset chargÃ© avec succÃ¨s !\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/tweets.csv\"\n",
    "if os.path.exists(data_path):\n",
    "    tweets = pd.read_csv(data_path, encoding = \"ISO-8859-1\")\n",
    "    print(\"âœ… Dataset chargÃ© avec succÃ¨s !\")\n",
    "else:\n",
    "    print(\"âŒ Le fichier tweets.csv est introuvable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Exploration & nettoyage\n",
    "## 2.1 - Nettoyage initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                                                                                               Non-Null Count    Dtype \n",
      "---  ------                                                                                                               --------------    ----- \n",
      " 0   0                                                                                                                    1599999 non-null  int64 \n",
      " 1   1467810369                                                                                                           1599999 non-null  int64 \n",
      " 2   Mon Apr 06 22:19:45 PDT 2009                                                                                         1599999 non-null  object\n",
      " 3   NO_QUERY                                                                                                             1599999 non-null  object\n",
      " 4   _TheSpecialOne_                                                                                                      1599999 non-null  object\n",
      " 5   @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tweets.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommage des colonnes\n",
    "tweets.columns = [\"label\", \"id\", \"date\", \"query\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes inutiles\n",
    "tweets = tweets.drop(columns=[\"id\", \"date\", \"query\", \"user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des labels (0 et 4 â†’ 0 et 1)\n",
    "tweets['label'] = tweets['label'].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Suppression des doublons sur la colonne \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Suppression des doublons effectuÃ©e : 18534 doublons supprimÃ©s.\n"
     ]
    }
   ],
   "source": [
    "before = len(tweets)\n",
    "tweets = tweets.drop_duplicates(subset = [\"text\"], keep = \"first\").reset_index(drop = True)\n",
    "after = len(tweets)\n",
    "print(f\"âœ… Suppression des doublons effectuÃ©e : {before - after} doublons supprimÃ©s.\")\n",
    "tweets = tweets.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Nettoyage avancÃ©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Nouveau run dÃ©marrÃ© : 8266a205d41145f982efa9c4aeb4448c\n",
      "âœ… Chargement des tweets nettoyÃ©s depuis ../models_saved/cleaned_tweets.pkl\n",
      "âœ… Run terminÃ©.\n"
     ]
    }
   ],
   "source": [
    "tweets_cleaned = dp.preprocess_tweets_parallel(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Vader scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Nouveau run dÃ©marrÃ© : f02e05a4cbbe449cb7c7a6ea72d5538b\n",
      "âœ… Scores VADER chargÃ©s depuis ../models_saved/vader_scores.pkl...\n",
      "\n",
      "ğŸ“Š Rapport de classification VADER :\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vader_scores \u001b[38;5;241m=\u001b[39m \u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_vader_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtweets_cleaned\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\notebooks\\../src\\utils.py:20\u001b[0m, in \u001b[0;36mmlflow_run_safety.<locals>.decorator.<locals>.wrapped_function\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸš€ Nouveau run dÃ©marrÃ© : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmlflow\u001b[38;5;241m.\u001b[39mactive_run()\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m     result \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Run terminÃ©.\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# âœ… SimplifiÃ© ici !\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\src\\data_preprocessing.py:83\u001b[0m, in \u001b[0;36mcompute_vader_scores\u001b[1;34m(df, text_column, save_path)\u001b[0m\n\u001b[0;32m     81\u001b[0m vader_pred \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ“Š Rapport de classification VADER :\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], vader_pred))\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# ğŸ“Œ (Optionnel) Log des metrics dans MLflow\u001b[39;00m\n\u001b[0;32m     86\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_score(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m], vader_pred)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "vader_scores = dp.compute_vader_scores(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Vectorisation des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 15000  # Taille Ã©chantillon USE (modifiable)\n",
    "\n",
    "tweets_sampled = tweets_cleaned.sample(n = sample_size, random_state = 70).reset_index(drop = True)\n",
    "\n",
    "X_bow, X_tfidf, X_fasttext, X_use, y_use = dp.vectorize_and_save(\n",
    "    tweets_cleaned['text'], tweets_sampled['text'],\n",
    "    tweets_cleaned['label'], tweets_sampled['label']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - ModÃ©lisation Classique (TF-IDF + RÃ©gression Logistique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, tweets_cleaned['label'], test_size = 0.2, random_state = 70, stratify=tweets_cleaned['label']\n",
    ")\n",
    "log_reg_model = mt.train_logistic_regression_with_cv(X_tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - ModÃ¨les AvancÃ©s (Random Forest / LightGBM / LSTM)\n",
    "## 6.1 - FastText + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft_train, X_ft_test, y_train, y_test = train_test_split(\n",
    "    X_fasttext, tweets_cleaned['label'], test_size = 0.2, random_state = 70, stratify = tweets_cleaned['label']\n",
    ")\n",
    "rf_model = mt.train_random_forest(X_ft_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - FastText + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model, (X_ft_test_reshaped, y_ft_test), history = mt.train_lstm_model(X_fasttext, tweets_cleaned['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - USE + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_use_train, X_use_test, y_use_train, y_use_test = train_test_split(\n",
    "    X_use, y_use, test_size = 0.2, random_state = 70, stratify = y_use\n",
    ")\n",
    "lgbm_model = mt.train_lightgbm(X_use_train, y_use_train, X_use_test, y_use_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 - DistilBERT\n",
    "### 6.4.1 - PrÃ©paration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = dp.prepare_distilbert_dataset(tweets_cleaned)\n",
    "tokenized = dp.tokenize_distilbert_dataset(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 - Fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, trainer, _ = mt.train_distilbert_model(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_acc, distilbert_f1 = ev.evaluate_distilbert_model(model, tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Comparaison finale des modÃ¨les"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'logreg': log_reg_model,\n",
    "    'rf': rf_model,\n",
    "    'lstm': lstm_model,\n",
    "    'lgbm': lgbm_model,\n",
    "    'distilbert_metrics': {\n",
    "        'accuracy': distilbert_acc,\n",
    "        'f1': distilbert_f1\n",
    "    }\n",
    "}\n",
    "\n",
    "datasets_dict = {\n",
    "    'tfidf': {'X_test': X_tfidf_test, 'y_test': y_test},\n",
    "    'fasttext': {'X_test': X_ft_test, 'y_test': y_test},\n",
    "    'lstm': (X_ft_test_reshaped, y_ft_test),\n",
    "    'use': {'X_test': X_use_test, 'y_test': y_use_test}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = ev.get_all_model_scores(models_dict, datasets_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Tracking MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = log_reg_model.predict(X_tfidf_test)\n",
    "mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "mlflow.log_metric(\"f1_score\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mlflow.active_run() is not None:\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    # entraÃ®nement modÃ¨le, log params & metrics\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A dÃ©sactiver en production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.launch_mlflow_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlflow.end_run()  # Pour stopper tout run en cours\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_p7_MLOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
