{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Chargement & configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\env_p7_MLOps\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\env_p7_MLOps\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "\n",
      "✅ Toutes les librairies sont présentes et prêtes à être utilisées !\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\motar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\motar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///c:/Users/motar/Desktop/1-openclassrooms/AI_Engineer/1-projets/P07/P7_sentiment_MLOps/notebooks/mlruns/906586012259731436', creation_time=1742576058928, experiment_id='906586012259731436', last_update_time=1742576058928, lifecycle_stage='active', name='Sentiment Analysis Project', tags={}>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "from requirements import *\n",
    "from src import data_preprocessing as dp\n",
    "from src import model_training as mt\n",
    "from src import evaluate as ev\n",
    "from src import utils\n",
    "\n",
    "import mlflow\n",
    "mlflow.set_experiment(\"Sentiment Analysis Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Affichage de la structure dossier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "├── .git\n",
      "├── ├── COMMIT_EDITMSG\n",
      "├── ├── FETCH_HEAD\n",
      "├── ├── HEAD\n",
      "├── ├── config\n",
      "├── ├── description\n",
      "├── ├── hooks\n",
      "├── ├── index\n",
      "├── ├── info\n",
      "├── ├── logs\n",
      "├── ├── objects\n",
      "├── └── refs\n",
      "├── .gitattributes\n",
      "├── .gitignore\n",
      "├── README.md\n",
      "├── data\n",
      "├── └── tweets.csv\n",
      "├── distilbert_model\n",
      "├── ├── config.json\n",
      "├── └── model.safetensors\n",
      "├── distilbert_output\n",
      "├── └── checkpoint-10000\n",
      "├── distilbert_results\n",
      "├── └── checkpoint-32\n",
      "├── env_p7_MLOps\n",
      "├── ├── Include\n",
      "├── ├── Lib\n",
      "├── ├── Scripts\n",
      "├── ├── etc\n",
      "├── ├── pyvenv.cfg\n",
      "├── └── share\n",
      "├── logs\n",
      "├── ├── events.out.tfevents.1741361244.PC-ARNAUD.37024.0\n",
      "├── ├── events.out.tfevents.1741362034.PC-ARNAUD.9628.0\n",
      "├── ├── events.out.tfevents.1741364804.PC-ARNAUD.38328.0\n",
      "├── ├── events.out.tfevents.1741507476.PC-ARNAUD.3024.0\n",
      "├── ├── events.out.tfevents.1741507646.PC-ARNAUD.3024.1\n",
      "├── ├── events.out.tfevents.1741513667.PC-ARNAUD.13340.0\n",
      "├── ├── events.out.tfevents.1741599815.PC-ARNAUD.27736.0\n",
      "├── ├── events.out.tfevents.1741603247.PC-ARNAUD.3852.0\n",
      "├── ├── events.out.tfevents.1741603852.PC-ARNAUD.24400.0\n",
      "├── ├── events.out.tfevents.1741604981.PC-ARNAUD.29024.0\n",
      "├── ├── events.out.tfevents.1741609885.PC-ARNAUD.30608.0\n",
      "├── ├── events.out.tfevents.1741633603.PC-ARNAUD.40828.0\n",
      "├── ├── events.out.tfevents.1741878760.PC-ARNAUD.26112.0\n",
      "├── ├── events.out.tfevents.1741896036.PC-ARNAUD.26288.0\n",
      "├── ├── events.out.tfevents.1741935846.PC-ARNAUD.7788.0\n",
      "├── ├── events.out.tfevents.1741941545.PC-ARNAUD.7788.2\n",
      "├── └── events.out.tfevents.1742199199.PC-ARNAUD.25320.0\n",
      "├── mlruns\n",
      "├── ├── .trash\n",
      "├── ├── 0\n",
      "├── ├── 346150508169324204\n",
      "├── └── 968426534980835769\n",
      "├── models\n",
      "├── models_saved\n",
      "├── ├── X_bow.pkl\n",
      "├── ├── X_fasttext.pkl\n",
      "├── ├── X_tfidf.pkl\n",
      "├── ├── X_use.pkl\n",
      "├── ├── cleaned_tweets.csv\n",
      "├── ├── cleaned_tweets.pkl\n",
      "├── ├── cosine_similarity.npy\n",
      "├── ├── distilbert_dataset.pkl\n",
      "├── ├── distilbert_eval_results.pkl\n",
      "├── ├── lgbm_model.txt\n",
      "├── ├── log_reg_model.pkl\n",
      "├── ├── lstm_eval_simulation.pkl\n",
      "├── ├── lstm_model.h5\n",
      "├── ├── rf_model.pkl\n",
      "├── ├── tfidf_split_data.joblib\n",
      "├── ├── tweets_fasttext.txt\n",
      "├── └── vader_scores.pkl\n",
      "├── notebooks\n",
      "├── ├── 01_full_pipeline_modeling.ipynb\n",
      "├── ├── P7_main_notebook.ipynb\n",
      "├── └── mlruns\n",
      "├── src\n",
      "├── ├── __init__.py\n",
      "├── ├── __pycache__\n",
      "├── ├── api\n",
      "├── ├── data_preprocessing.py\n",
      "├── ├── evaluate.py\n",
      "├── ├── functions.py\n",
      "├── ├── model_training.py\n",
      "├── ├── requirements.py\n",
      "├── ├── requirements.txt\n",
      "├── └── utils.py\n",
      "├── tests\n",
      "├── tmp_trainer\n",
      "├── └── runs\n",
      "└── tokenized_distilbert_dataset\n",
      "├── ├── cache-4fd5ed254c63e60d.arrow\n",
      "├── ├── cache-59508564038c63f4.arrow\n",
      "├── ├── cache-93488f2cd6523051.arrow\n",
      "├── ├── cache-b4c7a2967748401c.arrow\n",
      "├── ├── cache-bb6997e2954fe2b5.arrow\n",
      "├── ├── data-00000-of-00001.arrow\n",
      "├── ├── dataset_info.json\n",
      "├── └── state.json\n"
     ]
    }
   ],
   "source": [
    "utils.afficher_structure_dossier(\"..\", max_niveaux = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Répertoire courant : c:\\Users\\motar\\Desktop\\1-openclassrooms\\AI_Engineer\\1-projets\\P07\\P7_sentiment_MLOps\\notebooks\n"
     ]
    }
   ],
   "source": [
    "print(f\"📂 Répertoire courant : {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Dataset chargé avec succès !\n"
     ]
    }
   ],
   "source": [
    "data_path = \"../data/tweets.csv\"\n",
    "if os.path.exists(data_path):\n",
    "    tweets = pd.read_csv(data_path, encoding = \"ISO-8859-1\")\n",
    "    print(\"✅ Dataset chargé avec succès !\")\n",
    "else:\n",
    "    print(\"❌ Le fichier tweets.csv est introuvable.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Exploration & nettoyage\n",
    "## 2.1 - Nettoyage initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1599999 entries, 0 to 1599998\n",
      "Data columns (total 6 columns):\n",
      " #   Column                                                                                                               Non-Null Count    Dtype \n",
      "---  ------                                                                                                               --------------    ----- \n",
      " 0   0                                                                                                                    1599999 non-null  int64 \n",
      " 1   1467810369                                                                                                           1599999 non-null  int64 \n",
      " 2   Mon Apr 06 22:19:45 PDT 2009                                                                                         1599999 non-null  object\n",
      " 3   NO_QUERY                                                                                                             1599999 non-null  object\n",
      " 4   _TheSpecialOne_                                                                                                      1599999 non-null  object\n",
      " 5   @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  1599999 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(tweets.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renommage des colonnes\n",
    "tweets.columns = [\"label\", \"id\", \"date\", \"query\", \"user\", \"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes inutiles\n",
    "tweets = tweets.drop(columns=[\"id\", \"date\", \"query\", \"user\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des labels (0 et 4 → 0 et 1)\n",
    "tweets['label'] = tweets['label'].map({0: 0, 4: 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index\n",
    "tweets = tweets.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Nettoyage avancé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Nettoyage des tweets en cours...\n"
     ]
    }
   ],
   "source": [
    "tweets_cleaned = dp.preprocess_tweets_parallel(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Vader scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_scores = dp.compute_vader_scores(tweets_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Vectorisation des tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bow, X_tfidf, X_fasttext, X_use = dp.vectorize_tweets(tweets_cleaned['text'], tweets_cleaned['text'].sample(frac = 0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Modélisation Classique (TF-IDF + Régression Logistique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tfidf_train, X_tfidf_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, tweets_cleaned['label'], test_size = 0.2, random_state = 70, stratify=tweets_cleaned['label']\n",
    ")\n",
    "log_reg_model = mt.train_logistic_regression_with_cv(X_tfidf_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Modèles Avancés (Random Forest / LightGBM / LSTM)\n",
    "## 6.1 - FastText + Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ft_train, X_ft_test, y_train, y_test = train_test_split(\n",
    "    X_fasttext, tweets_cleaned['label'], test_size = 0.2, random_state = 70, stratify = tweets_cleaned['label']\n",
    ")\n",
    "rf_model = mt.train_random_forest(X_ft_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - FastText + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model, (X_ft_test_reshaped, y_ft_test), history = mt.train_lstm_model(X_fasttext, tweets_cleaned['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - USE + LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_use_train, X_use_test, y_train, y_test = train_test_split(\n",
    "    X_use, tweets_cleaned['label'], test_size = 0.2, random_state = 70, stratify = tweets_cleaned['label']\n",
    ")\n",
    "lgbm_model = mt.train_lightgbm(X_use_train, y_train, X_use_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 - DistilBERT\n",
    "### 6.4.1 - Préparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample = dp.prepare_distilbert_dataset(tweets_cleaned)\n",
    "tokenized = dp.tokenize_distilbert_dataset(df_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 - Fine-tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, trainer, _ = mt.train_distilbert_model(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilbert_acc, distilbert_f1 = ev.evaluate_distilbert_model(model, tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Comparaison finale des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'logreg': log_reg_model,\n",
    "    'rf': rf_model,\n",
    "    'lstm': lstm_model,\n",
    "    'lgbm': lgbm_model,\n",
    "    'distilbert_metrics': {\n",
    "        'accuracy': distilbert_acc,\n",
    "        'f1': distilbert_f1\n",
    "    }\n",
    "}\n",
    "\n",
    "datasets_dict = {\n",
    "    'tfidf': {'X_test': X_tfidf_test, 'y_test': y_test},\n",
    "    'fasttext': {'X_test': X_ft_test, 'y_test': y_test},\n",
    "    'lstm': (X_ft_test_reshaped, y_ft_test),\n",
    "    'use': {'X_test': X_use_test, 'y_test': y_test}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = ev.get_all_model_scores(models_dict, datasets_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Tracking MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run():\n",
    "    # entraînement modèle, log params & metrics\n",
    "    mlflow.log_param(\"model\", \"Logistic Regression\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_p7_MLOps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
